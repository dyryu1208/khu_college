{"cells":[{"cell_type":"markdown","metadata":{"id":"QtFr0M1S0NRT"},"source":["# Review Helpfulness Classification - BHelp-CoRT_SMALL\n","* Dataset - Amazon(Toys and Games, CDs and Vinyls)\n","* Features - Cleaned_Review_Text, Review_Rating\n","* BHelp-CoRT Model with small transformer layers(4 Layers, 512 Hidden sizes, 8 Attention heads)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEL_p6z60JKV"},"outputs":[],"source":[" !pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fl-sSGGa0afl"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dropout, Dense, Flatten, Conv1D, GlobalMaxPool1D, Input, concatenate, MaxPooling1D, GlobalMaxPooling1D, MaxPool1D, Concatenate, Multiply, Attention\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.activations import relu, sigmoid\n","from transformers import TFBertModel, BertTokenizer, BertConfig\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","import random\n","import nltk\n","import json\n","import tqdm\n","import re, os\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2MxKLAw0d-T"},"outputs":[],"source":["amzn = pd.read_csv('/datasets/datasets/preprocessed_amazon_Toys_and_Games.csv')\n","print(amzn.shape)\n","amzn.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhtJTQZf0fht"},"outputs":[],"source":["# Set Seed\n","# numpy와 tensorflow 2가지에 seed 설정\n","# 해당 코드 출처 : https://dacon.io/codeshare/2363\n","\n","def seed_everything(seed: int=42):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  os.environ['PYTHONASHSEED'] = str(seed)\n","  tf.random.set_seed(seed)\n","my_seed = 42\n","seed_everything(my_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoxrFZJw0ge6"},"outputs":[],"source":["amzn_train, amzn_test = train_test_split(amzn, test_size=0.2, shuffle=True, random_state=42, stratify=amzn['helpfulness label'])\n","train_rating = np.array(amzn_train['overall'])\n","test_rating = np.array(amzn_test['overall'])\n","\n","amzn_train_input_ids = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_train_input_ids.npy','rb'))\n","amzn_train_attention_masks = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_train_attention_masks.npy','rb'))\n","amzn_train_type_ids = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_train_type_ids.npy','rb'))\n","amzn_train_labels = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_train_label.npy','rb'))\n","\n","amzn_train_inputs = (amzn_train_input_ids, amzn_train_attention_masks, amzn_train_type_ids)\n","\n","amzn_test_input_ids = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_test_input_ids.npy','rb'))\n","amzn_test_attention_masks = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_test_attention_masks.npy','rb'))\n","amzn_test_type_ids = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_test_type_ids.npy','rb'))\n","amzn_test_labels = np.load(open('/datasets/bert_inputs/amzn_toys_and_games_cleaned_bert_test_label.npy','rb'))\n","\n","amzn_test_inputs = (amzn_test_input_ids, amzn_test_attention_masks, amzn_test_type_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8GUHsrc0hyP"},"outputs":[],"source":["# TPU 작동을 위한 셋업\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","strategy = tf.distribute.experimental.TPUStrategy(resolver)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Lvsfkh00i1I"},"outputs":[],"source":["# bert_tri_small\n","\n","class bert_tri_small(tf.keras.Model):\n","\n","  def __init__(self, model_name, dir_path, num_class):\n","    super(bert_tri_small, self).__init__()\n","\n","    self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path, from_pt=True)\n","    self.rating_emb = Embedding(6, 512, embeddings_regularizer=l2(), name='rating_embeddings')\n","    self.flat = Flatten()\n","    self.mul = Multiply()\n","    self.dropout = Dropout(self.bert.config.hidden_dropout_prob)\n","    self.clf = Dense(num_class,\n","                     activation='sigmoid',\n","                     name='classifier')\n","\n","  def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n","\n","    text_inputs, rating = inputs\n","    outputs = self.bert(text_inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","    rat = self.rating_emb(rating)\n","    rat = self.flat(rat)\n","    bert_output = outputs[1]\n","    bert_output = self.dropout(bert_output, training=training)\n","\n","    attention = Attention()([rat, bert_output])\n","    interaction = Multiply()([bert_output, attention])\n","\n","    drop_1 = self.dropout(interaction)\n","\n","    output = self.clf(drop_1)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"StCM59Y80naY"},"outputs":[],"source":["with strategy.scope():\n","  cls_model = bert_tri_small(model_name='nreimers/BERT-Small-L-4_H-512_A-8', dir_path='bert_ckpt', num_class=1)\n","  optimizer = Adam(1e-5)\n","  loss = tf.keras.losses.BinaryCrossentropy()\n","  metric = tf.keras.metrics.BinaryAccuracy()\n","  cls_model.compile(optimizer=optimizer,\n","                    loss=loss, metrics=[metric])\n","  es = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3)\n","  cp = ModelCheckpoint('bert_tri.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5L33GZ4E0n6A"},"outputs":[],"source":["result = cls_model.fit([amzn_train_inputs, train_rating], amzn_train_labels, batch_size=32, epochs=4, validation_split=0.2, callbacks=[es,cp])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kcO0__20o5n"},"outputs":[],"source":["def draw_plot(history,metric):\n","  plt.figure(figsize=(7,7))\n","  plt.plot(history.history[metric])\n","  plt.plot(history.history['val_'+metric])\n","  plt.title('Train / Valid Accuracy',fontsize=15)\n","  plt.ylabel(metric)\n","  plt.xlabel('Epochs')\n","  plt.legend([metric, 'val_'+metric])\n","  plt.show()\n","\n","draw_plot(result, 'binary_accuracy')\n","\n","f1_score_list = []\n","precision_list = []\n","recall_list = []\n","\n","y_pred = cls_model.predict([amzn_test_inputs, test_rating])\n","idx = 0\n","\n","for k in y_pred:\n","  if k >= 0.5:\n","    y_pred[idx] = 1\n","  else:\n","    y_pred[idx] = 0\n","  idx += 1\n","\n","f1_score_list.append(f1_score(amzn_test_labels, y_pred))\n","precision_list.append(precision_score(amzn_test_labels, y_pred))\n","recall_list.append(recall_score(amzn_test_labels, y_pred))\n","\n","#print(cls_model.evaluate([amzn_test_inputs, test_rating], amzn_test_labels, batch_size=32))\n","print(f'F1-Score : {f1_score_list}')\n","print(f'Precision : {precision_list}')\n","print(f'Recall : {recall_list}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNV8dAcNkUSBtUILVELIMYv","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
